# Linux Device Driver Evaluation System

## ğŸš€ Overview

This project evaluates the performance of AI coding models in generating Linux kernel device drivers. It benchmarks code generation against key metrics such as compilation success, functionality, security, and code quality.

## ğŸ“ Project Structure

```
linux_driver_eval_system/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ generated_code/        # Generated drivers by AI models
â”‚   â”œâ”€â”€ results/               # Evaluation outputs (JSON)
â”œâ”€â”€ docs/                      # Architecture and evaluation rubrics
â”œâ”€â”€ evaluation_engine.py       # Compilation and static checks
â”œâ”€â”€ metrics_calculator.py      # Weighted metric scoring
â”œâ”€â”€ run_evaluation.py          # Entry point for evaluation
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“¦ Dependencies

- `gcc`
- `clang-tidy`
- Python 3.7+
- Linux kernel headers (if compiling modules)

Install with:

```bash
sudo apt install gcc clang-tidy linux-headers-$(uname -r)
pip install -r requirements.txt
```

## â–¶ï¸ Run Evaluation

```bash
python run_evaluation.py
```

This evaluates the driver located at `app/generated_code/sample_driver.c` and outputs a JSON file in `app/results/`.

## ğŸ§  What It Measures

- Compilation success rate and diagnostics
- Static analysis warnings (clang-tidy)
- Functional placeholder scores
- Security checks (based on pattern matching)
- Code quality style and maintainability

## ğŸ“„ Documentation

See `docs/architecture.md` and `docs/rubric.md` for internals and grading system.
